# -*- coding: utf-8 -*-
"""Capstone (Breast Cancer Prediction)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JNGeJjMvZEj0oYk35nngztwhY6Xo7ij5

**Capstone - Breast cancer prediction using Deep Nueral Network**

---

**Background**

---



Breast cancer is a type of cancer with high mortality rates among women, and it is one of the most common causes of death in women. According to the National Cancer Institue statistics in USA, one out of eight women suffers from breast cancer and 6% of all deaths worldwide are caused by this type of cancer.

Early diagnosis and accurate diagnosis of breast cancer is of prime importance as it will increase the survival chances. Thus, a precise and reliable system is essential for the timely diagnosis of benign or malignant breast tumors.

**Objective**

---



The key objective is to develop a model to predict breast cancer as benign or malignant using the data set from the digitized image of FNA sample. 

Radiologists conduct Fine Needle Aspirate (FNA) procedure of breast tumor. FNA is a non invasive technique for detecting breast cancer. This procedure reveals features such as tumor radius, area, perimeter, concavity, texture and fractal dimensions. These features are further studied by medical experts to classify tumor as benign or malignant. Pathologists require a lot of skill and expertise to perform the analysis on the FNA sample. Applyting the suitable features of the FNA results in the most important diagnostic problem in early stages of breast cancer. Hence, development of algorithms which provide accurate predictions is of great interest.

---



Dataset used - Breast Cancer Wisconsin Diagnostic Data set

Features in the dataset are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.

Predictive Model - Deep Nueral Network using Tensor Flow
"""

# Commented out IPython magic to ensure Python compatibility.
# Predict whether the cancer is malignant or benign

# Load all the required packages

import tensorflow as tf
import pandas as pd
from sklearn.utils import shuffle
from sklearn import preprocessing
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
import matplotlib.gridspec as gridspec
import seaborn as sns
import matplotlib.pyplot as plt
import random as rn
import os
import numpy as np
# %matplotlib inline

"""Loading the dataset to the Colab notebook"""

# load the dataset from the local directory to the Colab jupyter notebook

from google.colab import files
uploaded = files.upload()

# Load the data

train_filename = "breast cancer data.csv"

# Set column keys

idKey = "id"
diagnosisKey = "diagnosis"
radiusMeanKey = "radius_mean"
textureMeanKey = "texture_mean"
perimeterMeanKey = "perimeter_mean"
areaMeanKey = "area_mean"
smoothnessMeanKey = "smoothness_mean"
compactnessMeanKey = "compactness_mean"
concavityMeanKey = "concavity_mean"
concavePointsMeanKey = "concave points_mean"
symmetryMeanKey = "symmetry_mean"
fractalDimensionMean = "fractal_dimension_mean"
radiusSeKey = "radius_se"
textureSeKey = "texture_se"
perimeterSeKey = "perimeter_se"
areaSeKey = "area_se"
smoothnessSeKey = "smoothness_se"
compactnessSeKey = "compactness_se"
concavitySeKey = "concavity_se"
concavePointsSeKey = "concave points_se"
symmetrySeKey = "symmetry_se"
fractalDimensionSeKey = "fractal_dimension_se"
radiusWorstKey = "radius_worst"
textureWorstKey = "texture_worst"
perimeterWorstKey = "perimeter_worst"
areaWorstKey = "area_worst"
smoothnessWorstKey = "smoothness_worst"
compactnessWorstKey = "compactness_worst"
concavityWorstKey = "concavity_worst"
concavePointsWorstKey = "concave points_worst"
symmetryWorstKey = "symmetry_worst"
fractalDimensionWorstKey = "fractal_dimension_worst"

train_columns = [idKey, diagnosisKey, radiusMeanKey, textureMeanKey, perimeterMeanKey, areaMeanKey, 
                 smoothnessMeanKey, compactnessMeanKey, concavityMeanKey, concavePointsMeanKey, 
                 symmetryMeanKey, fractalDimensionMean, radiusSeKey, textureSeKey, perimeterSeKey, 
                 areaSeKey, smoothnessSeKey, compactnessSeKey, concavitySeKey, concavePointsSeKey, 
                 symmetrySeKey, fractalDimensionSeKey, radiusWorstKey, textureWorstKey, perimeterWorstKey, 
                 areaWorstKey, smoothnessWorstKey, compactnessWorstKey, concavityWorstKey, 
                 concavePointsWorstKey, symmetryWorstKey, fractalDimensionWorstKey]

def get_train_data():
    df = pd.read_csv(train_filename, names= train_columns, delimiter=',', skiprows=1)
    return df

train_data = get_train_data()

# Exploring the data

train_data.head()

train_data.describe()

# Checking for null values

# No missing values found in the dataset

train_data.isnull().sum()

# how area_mean compares across malignant and benign diagnosis.

print ("Malignant")
print (train_data.area_mean[train_data.diagnosis == "M"].describe())
print ()
print ("Benign")
print (train_data.area_mean[train_data.diagnosis == "B"].describe())

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))

bins = 50

ax1.hist(train_data.area_mean[train_data.diagnosis == "M"], bins = bins)
ax1.set_title('Malignant')

ax2.hist(train_data.area_mean[train_data.diagnosis == "B"], bins = bins)
ax2.set_title('Benign')

plt.xlabel('Area Mean')
plt.ylabel('Number of Diagnosis')
plt.show()

"""The 'area_mean' feature looks different as it increases its value across both types of diagnosis. It appears that, the malignant diagnosis are more  uniformly distributed, while benign diagnosis have a normal distribution. This could make it easier to detect a malignant diagnosis when the area_mean is above the 750 value."""

# how the diagnosis area_worst differs between the two types.

print ("Malignant")
print (train_data.area_worst[train_data.diagnosis == "M"].describe())
print ()
print ("Benign")
print (train_data.area_worst[train_data.diagnosis == "B"].describe())

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))

bins = 30

ax1.hist(train_data.area_worst[train_data.diagnosis == "M"], bins = bins)
ax1.set_title('Malignant')

ax2.hist(train_data.area_worst[train_data.diagnosis == "B"], bins = bins)
ax2.set_title('Benign')

plt.xlabel('Area Worst')
plt.ylabel('Number of Diagnosis')
plt.yscale('log')
plt.show()

# Doesn't look much different than the last graph

#Selecting only the rest of the features

r_data = train_data.drop([idKey, areaMeanKey, areaWorstKey, diagnosisKey], axis=1)
r_features = r_data.columns

plt.figure(figsize=(12,28*4))
gs = gridspec.GridSpec(28, 1)
for i, cn in enumerate(r_data[r_features]):
    ax = plt.subplot(gs[i])
    sns.distplot(train_data[cn][train_data.diagnosis == "M"], bins=50)
    sns.distplot(train_data[cn][train_data.diagnosis == "B"], bins=50)
    ax.set_xlabel('')
    ax.set_title('histogram of feature: ' + str(cn))
plt.show()

# Update the value of diagnosis. 1 for Malignant and 0 for Benign

train_data.loc[train_data.diagnosis == "M", 'diagnosis'] = 1
train_data.loc[train_data.diagnosis == "B", 'diagnosis'] = 0

# Create a new feature for benign (non-malignant) diagnosis.

train_data.loc[train_data.diagnosis == 0, 'benign'] = 1
train_data.loc[train_data.diagnosis == 1, 'benign'] = 0

# Convert benign column type to integer

train_data['benign'] = train_data.benign.astype(int)

# Rename 'Class' to 'Malignant'

train_data = train_data.rename(columns={'diagnosis': 'malignant'})

# 212 malignant diagnosis, 357 benign diagnosis. 37.25% of diagnostics were malignant.

print(train_data.benign.value_counts())
print()
print(train_data.malignant.value_counts())

pd.set_option("display.max_columns",101)
train_data.head()

# Create dataframes of only Malignant and Benign diagnosis.

Malignant = train_data[train_data.malignant == 1]
Benign = train_data[train_data.benign == 1]

# Set train_X equal to 80% of the malignant diagnosis.

train_X = Malignant.sample(frac=0.8)
count_Malignants = len(train_X)

# Add 80% of the benign diagnosis to train_X.

train_X = pd.concat([train_X, Benign.sample(frac = 0.8)], axis = 0)

# test_X contains all the diagnostics not in train_X.

test_X = train_data.loc[~train_data.index.isin(train_X.index)]

# Shuffle the dataframes so that the training is done in a random order.

train_X = shuffle(train_X)
test_X = shuffle(test_X)

# Add the target features to train_Y and test_Y

train_Y = train_X.malignant
train_Y = pd.concat([train_Y, train_X.benign], axis=1)

test_Y = test_X.malignant
test_Y = pd.concat([test_Y, test_X.benign], axis=1)

# Drop target features from train_X and test_X

train_X = train_X.drop(['malignant','benign'], axis = 1)
test_X = test_X.drop(['malignant','benign'], axis = 1)

# Check to ensure all of the training/testing dataframes are of the correct length

print(len(train_X))
print(len(train_Y))
print(len(test_X))
print(len(test_Y))

# Names of all of the features in train_X

features = train_X.columns.values

# Transform each feature in features so that it has a mean of 0 and standard deviation of 1
# This helps with training the softmax algorithm

for feature in features:
    mean, std = train_data[feature].mean(), train_data[feature].std()
    train_X.loc[:, feature] = (train_X[feature] - mean) / std
    test_X.loc[:, feature] = (test_X[feature] - mean) / std

"""Train the Neural Network"""

# Parameters

learning_rate = 0.005
training_dropout = 0.9
display_step = 1
training_epochs = 5
batch_size = 100
accuracy_history = [] 
cost_history = []
valid_accuracy_history = [] 
valid_cost_history = []

# Number of input nodes

input_nodes = train_X.shape[1]

# Number of labels (malignant and benign)

num_labels = 2

# Split the testing data into validation and testing sets

split = int(len(test_Y)/2)

train_size = train_X.shape[0]
n_samples = train_Y.shape[0]

input_X = train_X.to_numpy()
input_Y = train_Y.to_numpy()
input_X_valid = test_X.to_numpy()[:split]
input_Y_valid = test_Y.to_numpy()[:split]
input_X_test = test_X.to_numpy()[split:]
input_Y_test = test_Y.to_numpy()[split:]

def calculate_hidden_nodes(nodes):
    return (((2 * nodes)/3) + num_labels)

# Number of nodes in each hidden layer

hidden_nodes1 = round(calculate_hidden_nodes(input_nodes))
hidden_nodes2 = round(calculate_hidden_nodes(hidden_nodes1))
hidden_nodes3 = round(calculate_hidden_nodes(hidden_nodes2))
print(input_nodes, hidden_nodes1, hidden_nodes2, hidden_nodes3)

# Percent of nodes to keep during dropout.

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() # solution to overcome the placeholder issue in Tensorflow 2.0

pkeep = tf.placeholder(tf.float32)

# Input

x = tf.placeholder(tf.float32, [None, input_nodes])

# Layer 1

W1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes1], stddev = 0.1))
b1 = tf.Variable(tf.zeros([hidden_nodes1]))
y1 = tf.nn.relu(tf.matmul(x, W1) + b1)

# Layer 2

W2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev = 0.1))
b2 = tf.Variable(tf.zeros([hidden_nodes2]))
y2 = tf.nn.relu(tf.matmul(y1, W2) + b2)

# Layer 3

W3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev = 0.1)) 
b3 = tf.Variable(tf.zeros([hidden_nodes3]))
y3 = tf.nn.relu(tf.matmul(y2, W3) + b3)
y3 = tf.nn.dropout(y3, pkeep)

# Layer 4

W4 = tf.Variable(tf.truncated_normal([hidden_nodes3, 2], stddev = 0.1)) 
b4 = tf.Variable(tf.zeros([2]))
y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)

# Output

y = y4
y_ = tf.placeholder(tf.float32, [None, num_labels])

# Minimize error using cross entropy
# Adam optimiser

import datetime

cost = -tf.reduce_sum(y_ * tf.log(y))

optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Test the model

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

# Calculate accuracy

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# Initializing the variables

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
for epoch in range(training_epochs): 
  for batch in range(int(n_samples/batch_size)):
    batch_x = input_X[batch * batch_size : (1 + batch) * batch_size]
    batch_y = input_Y[batch * batch_size : (1 + batch) * batch_size]

sess.run([optimizer], feed_dict={x: batch_x, 
                                 y_: batch_y,
                                 pkeep: training_dropout})

# Display logs

if (epoch) % display_step == 0:
  train_accuracy, newCost = sess.run([accuracy, cost], 
                                     feed_dict={x: input_X, y_: input_Y, 
                                     pkeep: training_dropout})

valid_accuracy, valid_newCost = sess.run([accuracy, cost], 
                                          feed_dict={x: input_X_valid, 
                                          y_: input_Y_valid, pkeep: 1})

print ("Epoch:", epoch, "Acc =", "{:.5f}".format(train_accuracy), 
       "Cost =", "{:.5f}".format(newCost), 
       "Valid_Acc =", "{:.5f}".format(valid_accuracy), 
       "Valid_Cost = ", "{:.5f}".format(valid_newCost))
            
# Record the results of the model

accuracy_history.append(train_accuracy)
cost_history.append(newCost)
valid_accuracy_history.append(valid_accuracy)
valid_cost_history.append(valid_newCost)
            
# If the model does not improve after 15 logs, stop the training

if valid_accuracy < max(valid_accuracy_history) and epoch > 100:
  stop_early += 1
      if stop_early == 15:
        break
            else:
              stop_early = 0

            
print("Optimization Finished!")

# Plot the accuracy and cost summaries 
f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))

ax1.plot(accuracy_history, color='b') # blue
ax1.plot(valid_accuracy_history, color='g') # green
ax1.set_title('Accuracy')

ax2.plot(cost_history, color='b')
ax2.plot(valid_cost_history, color='g')
ax2.set_title('Cost')

plt.xlabel('Epochs (x10)')
plt.show()

"""**Conclusion**

---

This is the best fit model in terms of accuracy. Using the 'AdamOptimizer' with all of the features in the Neural Network, the model gives a prediction accuracy of 96% and a cross-validation score of 96% for the test data set. This model performs reasonably well.
"""